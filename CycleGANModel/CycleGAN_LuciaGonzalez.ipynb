{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOUrtHC4WeZU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CycleGAN\n",
    "\n",
    "CycleGAN is a model that aims to solve the image-to-image translation\n",
    "problem. The goal of the image-to-image translation problem is to learn the\n",
    "mapping between an input image and an output image using a training set of\n",
    "aligned image pairs. However, obtaining paired examples isn't always feasible.\n",
    "CycleGAN tries to learn this mapping without requiring paired input-output images,\n",
    "using cycle-consistent adversarial networks.\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1703.10593.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hTHTkBYWeZV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEz-w8XyWeZW",
    "outputId": "a8bda644-bc20-4c72-89e5-ddcd18f8d1ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np                \n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "autotune = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet imports the following libraries: os, numpy (renamed as np), matplotlib.pyplot (renamed as plt), tensorflow (renamed as tf), and tensorflow\\_addons (renamed as tfa). It also configures the autotune variable to use TensorFlow's \"AUTOTUNE\" option.\n",
    "\n",
    "*   `os`: Provides functions for interacting with the operating system, particularly for file and directory-related operations.\n",
    "*   `numpy`: A popular library for numerical computations in Python, commonly used for manipulating arrays and performing efficient mathematical operations.\n",
    "*   `matplotlib.pyplot`: A visualization library in Python that provides a MATLAB-like interface for creating plots and visualizations.\n",
    "*   `tensorflow`: An open-source machine learning library developed by Google. It is used for building and training machine learning models, especially neural networks.\n",
    "*   `tensorflow.keras.layers`: A sub-module of TensorFlow's Keras API that offers pre-defined layers used for constructing neural network models.\n",
    "*   `tensorflow_addons`: An additional library for TensorFlow that provides extra implementations of advanced algorithms and layers to enhance and extend TensorFlow's capabilities.\n",
    "*   `autotune`: A variable set to utilize TensorFlow's \"AUTOTUNE\" option, which automatically selects the best performance configuration based on the execution context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FDRF_eOWeZY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prepare the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLa1-g38TYDI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the path of our local dataset\n",
    "dataset_path = \"./Data\"\n",
    "\n",
    "# Function to obtain image paths\n",
    "def get_image_paths(dataset_dir):\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image_paths.append(image_path)\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "# Obtain the routes of the training and test images.\n",
    "train_human_paths = get_image_paths(os.path.join(dataset_path, \"trainA\"))\n",
    "train_anime_paths = get_image_paths(os.path.join(dataset_path, \"trainB\"))\n",
    "test_human_paths = get_image_paths(os.path.join(dataset_path, \"testA\"))\n",
    "test_anime_paths = get_image_paths(os.path.join(dataset_path, \"testB\"))\n",
    "\n",
    "# Create the datasets for training and testing\n",
    "train_human_ds = tf.data.Dataset.from_generator(lambda: train_human_paths, output_types=tf.string)\n",
    "train_anime_ds = tf.data.Dataset.from_generator(lambda: train_anime_paths, output_types=tf.string)\n",
    "test_human_ds = tf.data.Dataset.from_generator(lambda: test_human_paths, output_types=tf.string)\n",
    "test_anime_ds = tf.data.Dataset.from_generator(lambda: test_anime_paths, output_types=tf.string)\n",
    "\n",
    "\n",
    "\n",
    "# Define the standard image size.\n",
    "orig_img_size = (158, 158)\n",
    "# Size of the random crops to be used during training.\n",
    "input_img_size = (128, 128, 3)\n",
    "# Weights initializer for the layers.\n",
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# Gamma initializer for instance normalization.\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "buffer_size = 128\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "\n",
    "def preprocess_train_image(path):\n",
    "    img = tf.io.read_file(path)                               \n",
    "    img = tf.image.decode_jpeg(img, channels=3)               \n",
    "\n",
    "    # Random flip\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    # Resize to the original size first\n",
    "    img = tf.image.resize(img, [*orig_img_size])\n",
    "    # Random crop to 256X256\n",
    "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
    "    # Normalize the pixel values in the range [-1, 1]\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_test_image(path):   \n",
    "    img = tf.io.read_file(path)                               \n",
    "    img = tf.image.decode_jpeg(img, channels=3)               \n",
    "\n",
    "    # Only resizing and normalization for the test images.\n",
    "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
    "    img = normalize_img(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fragment is responsible for loading and preparing images for use in the training and testing process of the machine learning model.  First, image paths are obtained within a specific directory and datasets are created using the obtained image paths. These datasets are used for training and testing the model. Then, image preprocessing functions are defined for training and testing. These functions read the images from the paths, apply preprocessing operations such as random flipping, resizing and normalization, and return the preprocessed images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OxOYMY-WeZa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create `Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fA0oHXsvUYhH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the preprocessing operations to the training data\n",
    "train_human = (\n",
    "    train_human_ds.map(preprocess_train_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "train_anime = (\n",
    "    train_anime_ds.map(preprocess_train_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n",
    "# Apply the preprocessing operations to the test data\n",
    "test_human = (\n",
    "    test_human_ds.map(preprocess_test_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "test_anime = (\n",
    "    test_anime_ds.map(preprocess_test_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the preprocessing operations are applied together with the \"cache()\" method to cache the data in memory to improve performance and the \"shuffle(buffer_size)\" method to randomize the data with a specified buffer size. Finally, the data is batched (batch_size) using the \"batch()\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-raPfixWeZd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XOjYtm_DWeZe",
    "outputId": "0286a47b-b86e-4c2f-e2d2-04d2af7b0070",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
    "for i, samples in enumerate(zip(train_human.take(4), train_anime.take(4))):\n",
    "    human = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
    "    anime = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n",
    "    ax[i, 0].imshow(human)\n",
    "    ax[i, 1].imshow(anime)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fragment shows some examples of the images in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqeQAZ3kWeZe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Building blocks used in the CycleGAN generators and discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONCN-UkcWeZf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ReflectionPadding2D(layers.Layer):\n",
    "    \"\"\"Implements Reflection Padding as a layer.\n",
    "\n",
    "    Args:\n",
    "        padding(tuple): Amount of padding for the\n",
    "        spatial dimensions.\n",
    "\n",
    "    Returns:\n",
    "        A padded tensor with the same type as the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
    "\n",
    "\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fragment defines several functions and a class to implement the CycleGAN style neural network architecture.\n",
    "\n",
    "The ReflectionPadding2D class implements the reflection padding method in two dimensions. This layer is used to add the reflection padding to the convolution input, which allows to better preserve the image features.\n",
    "\n",
    "The residual_block function implements a residual block of the neural network. Residual blocks are important to help the network learn complex nonlinear transformations. In this case, two 2D convolutions with a ReLU activation function and an instance normalization layer are used. The reflection padding layer is also applied to the residual block input.\n",
    "\n",
    "The downsample and upsample functions implement down-sample and up-sample layers, respectively. Downsampling is used to reduce the image size and increase the number of image features, while upsampling is used to increase the image size and reduce the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9pT7tGpWeZf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build the generators\n",
    "\n",
    "The generator consists of downsampling blocks: nine residual blocks\n",
    "and upsampling blocks. The structure of the generator is the following:\n",
    "\n",
    "```\n",
    "c7s1-64 ==> Conv block with `relu` activation, filter size of 7\n",
    "d128 ====|\n",
    "         |-> 2 downsampling blocks\n",
    "d256 ====|\n",
    "R256 ====|\n",
    "R256     |\n",
    "R256     |\n",
    "R256     |\n",
    "R256     |-> 9 residual blocks\n",
    "R256     |\n",
    "R256     |\n",
    "R256     |\n",
    "R256 ====|\n",
    "u128 ====|\n",
    "         |-> 2 upsampling blocks\n",
    "u64  ====|\n",
    "c7s1-3 => Last conv block with `tanh` activation, filter size of 7.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaS-3_fJWeZg",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = keras.models.Model(img_input, x, name=name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function starts by creating an image input using the Keras Input class. Then, a reflection padding is performed on the image using the ReflectionPadding2D class. A 7x7 convolutional layer with a number of filters is applied, followed by instance normalization and ReLU activation.\n",
    "\n",
    "Then, a number of downsampling blocks are applied, each reducing the spatial resolution of the image by half. After the downsampling blocks, a number of residual blocks are applied to allow for connection hops in the network.\n",
    "\n",
    "After the residual blocks, a number of upsampling blocks are applied, each doubling the spatial resolution of the image. Finally, a final padding reflection block is applied, followed by a 7x7 convolutional layer with 3 filters and a hyperbolic tangent activation (tanh).\n",
    "\n",
    "The model is compiled using the image input and the output generated by the last convolutional layer, and returned as an instance of the Keras Model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0X2IRDcWeZg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build the discriminators\n",
    "\n",
    "The discriminators implement the following architecture:\n",
    "`C64->C128->C256->C512`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XK9iAUd8WeZg",
    "outputId": "891cff9b-a234-4a74-b42e-d1e7d8d5193a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        (4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(1, 1),\n",
    "            )\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(name=\"generator_G\")\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(name=\"discriminator_Y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fragment defines a function to create a discriminator that takes an input image and produces a single-valued output indicating whether the image is real or false. The discriminator is constructed using a series of convolutional and down-sampling layers that reduce the spatial resolution of the image and increase the number of channels. Finally, a 1x1 convolution layer is used to produce a single-valued output.\n",
    "\n",
    "After constructing the get_discriminator function, the code also defines two ResNet generators, gen_G and gen_F, using the get_resnet_generator function. Next, the code creates two discriminators, disc_X and disc_Y, using the get_discriminator function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJfnzw-YWeZh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build the CycleGAN model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Si-YvJk0WeZh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return (\n",
    "            self.disc_X(inputs),\n",
    "            self.disc_Y(inputs),\n",
    "            self.gen_G(inputs),\n",
    "            self.gen_F(inputs),\n",
    "        )\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is Human and y is anime\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real images through the generators and get the generated images\n",
    "        # 2. Pass the generated images back to the generators to check if we\n",
    "        #    we can predict the original image from the generated image.\n",
    "        # 3. Do an identity mapping of the real images using the generators.\n",
    "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Human to fake anime\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            # Anime to fake human -> y2x\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle (Human to fake anime to fake human): x -> y -> x\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            # Cycle (Anime to fake human to fake anime) y -> x -> y\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adverserial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet defines a CycleGAN model in TensorFlow as a subclass of keras.Model. The class constructor initializes the generators (gen_G and gen_F), discriminators (disc_X and disc_Y), and other parameters such as lambda values for cycle loss and identity loss. The \"call\" method defines the forward pass of the model, where it passes the input through the discriminators and generators, returning the outputs. The \"compile\" method sets up the optimizers and loss functions for training the model. The \"train_step\" method defines a single training step, where it performs operations for training the CycleGAN model. This includes generating fake images, calculating losses for generators and discriminators, calculating gradients, and updating the weights of the networks. And finally, the method returns a dictionary containing the calculated losses during the training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfS-EBdrWeZi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a callback that periodically saves generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1CTqjIcWeZi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
    "\n",
    "    def __init__(self, num_img=4):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n",
    "        for i, img in enumerate(test_human.take(self.num_img)):\n",
    "            prediction = self.model.gen_G(img)[0].numpy()\n",
    "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "            ax[i, 0].imshow(img)\n",
    "            ax[i, 1].imshow(prediction)\n",
    "            ax[i, 0].set_title(\"Input image\")\n",
    "            ax[i, 1].set_title(\"Translated image\")\n",
    "            ax[i, 0].axis(\"off\")\n",
    "            ax[i, 1].axis(\"off\")\n",
    "\n",
    "            prediction = keras.preprocessing.image.array_to_img(prediction)\n",
    "            prediction.save(\n",
    "                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n",
    "            )\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a method that is executed at the end of each epoch during the training of the deep learning model. The method generates a translated image (prediction) from each input image (img), visualizes the images generated by the model during the training process and saves each generated image in a PNG file using the Keras save() function. This allows the user to observe how the model is improving as the training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UL4gc0D_WeZi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the end-to-end model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xvyu-0WHWeZj",
    "outputId": "da843359-a5cd-41fb-aeba-2100859a73d4",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=keras.optimizers.legacy.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn,\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we will train the model for 30 epochs.\n",
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((train_human, train_anime)),\n",
    "    epochs=30,\n",
    "    callbacks=[plotter, model_checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fragment defines the adversarial loss function and the loss functions of the generators and discriminators. A cycle_gan_model object representing the CycleGAN model is created and compiled, using the previously defined generators and discriminators, and callbacks are defined to display the images generated during training and to store the model weights during training. Finally, the cycle_gan_model model is trained using the training data (train_human and train_anime) for a number of epochs, in this example 30.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjsyuE65WeZj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoMuUaIgWeZk",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the checkpoints\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "cycle_gan_model.load_weights(latest).expect_partial()\n",
    "print(\"Weights loaded successfully\")\n",
    "\n",
    "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
    "for i, img in enumerate(test_human.take(4)):\n",
    "    prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()\n",
    "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
    "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
    "\n",
    "    ax[i, 0].imshow(img)\n",
    "    ax[i, 1].imshow(prediction)\n",
    "    ax[i, 0].set_title(\"Input image\")\n",
    "    ax[i, 1].set_title(\"Translated image\")\n",
    "    ax[i, 0].axis(\"off\")\n",
    "    ax[i, 1].axis(\"off\")\n",
    "\n",
    "    prediction = keras.preprocessing.image.array_to_img(prediction)\n",
    "    prediction.save(\"predicted_img_{i}.png\".format(i=i))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this code fragment loads the previously saved trained model weights using tf.train.latest_checkpoint to obtain the path to the most recent weights file, generates and displays the translated images using the CycleGAN model, and saves the translated images as PNG files."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
